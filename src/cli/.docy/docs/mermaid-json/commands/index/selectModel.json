{
  "fileName": "selectModel.ts",
  "filePath": "commands/index/selectModel.ts",
  "mermaidSummary": "```mermaid\ngraph TD\n    A[Start: selectModel Function] --> B{Check Priority}\n    B -->|COST| C[Check for GPT3 Model]\n    B -->|OTHER| D[Check for GPT4 Model]\n    C -->|Exists| E{Check GPT3 Max Length}\n    C -->|Does Not Exist| F[Check for GPT4 Model]\n    E -->|Sufficient| G[Return GPT3 Model Details]\n    E -->|Insufficient| F\n    F -->|Exists| H{Check GPT4 Max Length}\n    F -->|Does Not Exist| I[Check for GPT432k Model]\n    H -->|Sufficient| J[Return GPT4 Model Details]\n    H -->|Insufficient| I\n    I -->|Exists| K{Check GPT432k Max Length}\n    I -->|Does Not Exist| L[Return null]\n    K -->|Sufficient| M[Return GPT432k Model Details]\n    K -->|Insufficient| L\n    D -->|Exists| N{Check GPT4 Max Length on Other Priority}\n    D -->|Does Not Exist| O[Return GPT4turbo Model Details]\n    N -->|Sufficient| P[Return GPT4 Model Details on Other Priority]\n    N -->|Insufficient| Q[Check for GPT432k Model on Other Priority]\n    Q -->|Exists| R{Check GPT432k Max Length on Other Priority}\n    Q -->|Does Not Exist| S[Return null on Other Priority]\n    R -->|Sufficient| T[Return GPT432k Model Details on Other Priority]\n    R -->|Insufficient| S\n\n    subgraph getMaxPromptLength\n        direction TB\n        AA[Start: getMaxPromptLength Function] --> BB{Encode Prompts}\n        BB --> CC[Return Max Prompt Length]\n    end\n\n    E --> AA\n    H --> AA\n    K --> AA\n    N --> AA\n    R --> AA\n```\nThis mermaid diagram illustrates the flow of the `selectModel` function, detailing how it decides which model to return based on priority and the sufficiency of the model's maximum length compared to the encoded prompt lengths. The `getMaxPromptLength` function is represented as a subgraph to show its role in calculating the maximum length of the encoded prompts, which is a crucial step in the decision-making process of the `selectModel` function.",
  "checksum": "33e40b0c0478e35a504c922288a76694"
}